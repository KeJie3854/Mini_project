{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c00aa5-19d1-4ab3-8ca1-02f3ba5eb324",
   "metadata": {},
   "source": [
    "# Tutorial week 4 \n",
    "\n",
    "# Learning outcomes\n",
    "1. Gentle introduction to what is computer vision\n",
    "2. Review: Important concepts of image stored as Numpy array\n",
    "3. Image cropping\n",
    "4. Different types of color models: HSV, RGB and CIELAB\n",
    "    * Split and merge image channels\n",
    "    * Manipulate the image channels\n",
    "5. Point operators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba8fd3-83ac-4a95-884e-aad7c124791d",
   "metadata": {},
   "source": [
    "# What is digital image processing / computer vision?\n",
    "\n",
    "As humans, we perceive the 3D structure of the world around us with ease. For example, looking at a framed group portrait, you can easily count and name all the people in the picture and even guess at their emotions from their facial expressions.\n",
    "\n",
    "Perceptual psychologists have spent decades trying to comprehend how visual system works and optical illusions have been discovered to solve the puzzle, a complete solution is still far beyond our reach. \n",
    "\n",
    "Computer vision / digital image processing is being utilized in diverse of real world applications:\n",
    "- Optical character recognition (OCR): reading handwritten postal codes on letters and automatic plate recognition.\n",
    "\n",
    "  ![OCR](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Portable_scanner_and_OCR_%28video%29.webm/1200px--Portable_scanner_and_OCR_%28video%29.webm.jpg \"Optical character recognition\")\n",
    "- Medical imaging: registering pre-operative and intra-operative imagery or performing long term studies of internal organ.\n",
    "\n",
    "  ![CT scan](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRta2V0mlQ-4oVHOUfyhRGpyPm64T4smphtzg&s \"CT scans\")\n",
    "- Self-driving vehicles.\n",
    "\n",
    "  ![Autonomous driving](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQdDERoAxq7k7ujdso6ghyI2hm6yn2cK9pJAQ&s \"autonomous vehicle\")\n",
    "- Surveillance: monitoring for intruders, analyzing highway traffic and monitoring pools for drowning victims.\n",
    "\n",
    "  ![surveillance](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSP9mr_ytNAapxlWafQLG5AcZZKVZ2wgPheFQ&s \"surveillance system\")\n",
    "- Fingerprint recognition and biometrics: automatic access authentication as well as forensic applications.\n",
    "\n",
    "  ![biometrics](https://www.nec.co.nz/wp-content/uploads/2018/02/Close-up-of-womans-left-eye-showing-iris-recogntion-points-market.jpg \"iris recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ea280-c656-4771-9f55-c84757ba5b82",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b14005-c4bd-43e6-b3b1-129b888bed35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display_images' from 'utils' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcv\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_images\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Make sure that optimization is enabled\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cv\u001b[38;5;241m.\u001b[39museOptimized():\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'display_images' from 'utils' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Python 3.8 is required\n",
    "assert sys.version_info >= (3, 8)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from utils import display_images\n",
    "\n",
    "# Make sure that optimization is enabled\n",
    "if not cv.useOptimized():\n",
    "    cv.setUseOptimized(True)\n",
    "    \n",
    "cv.useOptimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa2bd5-33d5-4aa5-9867-071f5abdc953",
   "metadata": {},
   "source": [
    "# Review: Representation of image as Numpy array\n",
    "An image is a multidimensional array; it has columns and rows of pixels, and each pixel has a value. For different kinds of image data, the pixel value may be formatted in different ways. We can create a $4\\times 4$ square black image from scratch by simply creating a 2D NumPy array as shown in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46498af3-72df-4eb9-bb84-c31c850c7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((4, 4), dtype = np.uint8)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659d1a7-089d-48e8-b863-e3af23c48d01",
   "metadata": {},
   "source": [
    "Here, each pixel is represented by a single 8-bit integer, which means that the values of each pixel are in 0-255 range, where 0 is black, 255 is white and the in-between values are shades of gray. This is a **grayscale** image. You can use `cv.cvtColor()` to convert the images from one color space to another. We will discuss about image color spaces later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b732522a-9e39-4117-a881-53d912566ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "img_bgr = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "print(img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861d4c4e-bae0-49be-8a54-626fdc705550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(img_bgr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35e707-c028-4d0b-8c48-0b0a56804e2d",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "---\n",
    "1. Create a $200 \\times 200$ white image and display it. \n",
    "2. Leverage your image processing skills to create a simple wallpaper design as shown in the following image:\n",
    "\n",
    "![pattern](img_embed/exercise_w4.jpg \"Pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63690352-ffef-4565-8dc0-f04e1592454b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#q1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m white \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(white, cmap\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mgray, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "white = np.ones((200, 200), dtype=\"uint8\") * 255\n",
    "plt.imshow(white, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "plt.title(\"White Image\")\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8baba6d-07f9-4671-95bc-cfa951fadf00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#replicate the patch by 3 times across the width and height\u001b[39;00m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(patch, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(patch)\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "patch = np.zeros((30,30), dtype=np.uint8)\n",
    "\n",
    "patch[:10, 10:20] = 255\n",
    "patch[10:20, :10] = 255\n",
    "patch[10:20, 20:] = 255\n",
    "patch[20:, 10:20] = 255\n",
    "\n",
    "#replicate the patch by 3 times across the width and height\n",
    "img = np.tile(patch, (3, 3))\n",
    "\n",
    "plt.imshow(patch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead7283-fed0-4e83-b4b4-8b580abb2e10",
   "metadata": {},
   "source": [
    "## Access elements in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f448bb-9739-404b-ba7e-976b538c3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 ns ± 3.51 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
      "120 ns ± 4.63 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "img = cv.imread('images/lena.jfif')\n",
    "\n",
    "%timeit a = img[100, 30, 0]\n",
    "%timeit b = img.item(100, 30, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486084d-a7ef-498d-8317-4d980653d7f5",
   "metadata": {},
   "source": [
    "## Numpy array slicing\n",
    "### Exercise\n",
    "2. Extract the region of interest (flower) from the 'flower.jfif'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2c05aa0-4065-4a66-add1-4cb1afc8cae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display_image' from 'utils' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m img \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/flower.jfif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_image\n\u001b[0;32m      5\u001b[0m display_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, img)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'display_image' from 'utils' (C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "img = cv.imread(\"images/flower.jfif\")\n",
    "\n",
    "from utils import display_image\n",
    "\n",
    "display_image(\"images\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487fc66-444d-434c-ba3a-c017ce847ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images/flower.jfif\")\n",
    "\n",
    "def mouse(event, x, y, flags, params):\n",
    "    if event == cv.EVENT_LBUTTONDOWN: # LEFT CLICK\n",
    "        print(x, \",\", y)\n",
    "        cv.imshow(\"image\", img)\n",
    "        \n",
    "cv.imshow(\"image\", img)\n",
    "cv.setMouseCallback(\"image\", mouse)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f9273-c11e-42d1-ae49-c5f1c94b4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower = img[57:114, 96:170]\n",
    "\n",
    "display_image(\"flower\", flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421beff-837a-464d-b233-c6a957e4ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cv.selectROI(\"img\", img)\n",
    "\n",
    "print(r)\n",
    "cv.destroyAllWindows() #(x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e5e94-f71e-460f-ac8c-365a157a67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,w,h = r\n",
    "\n",
    "flower = img[y: y+h, x: x+w]\n",
    "\n",
    "display_image(\"flower\", flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa233bae-a91c-4d66-8627-e544b0d6e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from chatgpt\n",
    "import cv2 as cv\n",
    "\n",
    "# Load the image\n",
    "img = cv.imread(\"images/flower.jfif\")\n",
    "\n",
    "# Open a window and select the ROI\n",
    "r = cv.selectROI(\"img\", img)\n",
    "\n",
    "# Print the coordinates of the selected ROI\n",
    "print(\"Selected ROI:\", r)  # (x, y, w, h)\n",
    "\n",
    "# Close the ROI selection window\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Extract the ROI from the image using the coordinates\n",
    "x, y, w, h = r\n",
    "roi = img[y:y+h, x:x+w]\n",
    "\n",
    "# Display the selected ROI in a new window\n",
    "cv.imshow(\"Selected ROI\", roi)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667dbc5-74ae-4618-9434-41a807ecda69",
   "metadata": {},
   "source": [
    "# Cropping an image\n",
    "Why crop an image? \n",
    "1. Remove all unwanted objects or areas from an image\n",
    "2. Improve the overall composition of the image. Visit this [link](https://expertphotography.com/improve-your-composition-the-rule-of-thirds/) on how separating image into grids and putting the subject of interest on the intersection point could create a more compositional pleasing photo. This is known as of rule of thirds.\n",
    "3. One of the image augmentation techniques in deep learning model training.\n",
    "\n",
    "The operations are literally the same as extracting ROI.\n",
    "\n",
    "## Divide an image into smaller patches using cropping\n",
    "One practical application of cropping in OpenCV is to divide an image into smaller patches. The following example shows how to split image into a $2 \\times 3$ grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415b308-8cff-4444-884c-c304e90d075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('images/dog.jfif') #load image\n",
    "img_copy = img.copy()\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "num_vertical_patches = 2\n",
    "num_horizontal_patches = 3\n",
    "\n",
    "# M and N are basically number of pixels per patch\n",
    "M, N = int(height / num_vertical_patches), int(width / num_horizontal_patches)\n",
    "\n",
    "x1, y1 = 0, 0\n",
    "\n",
    "for y in range(0, height, M):\n",
    "    for x in range(0, width, N):\n",
    "        \n",
    "        y1 = y + M\n",
    "        x1 = x + N\n",
    "        \n",
    "        if x1>=width and y1>=height:\n",
    "            x1 = width-1\n",
    "            y1 = height-1\n",
    "            tile = img[y:height, x:width]\n",
    "            cv.rectangle(img_copy, (x,y), (x1, y1), (0, 255, 0), 1)\n",
    "            cv.imshow('tile', tile)\n",
    "            \n",
    "        elif y1>=height:\n",
    "            y1 = height-1\n",
    "            cv.rectangle(img_copy, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "            \n",
    "        elif x1>=width:\n",
    "            x1 = width-1\n",
    "            cv.rectangle(img_copy, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "            \n",
    "        else:\n",
    "            cv.rectangle(img_copy, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "            \n",
    "cv.imshow('patched image', img_copy)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea736b7-ef45-4d71-bb84-e8f5635a9fd6",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Divide the image into 4 equal regions. Swap their positions as shown below:\n",
    "\n",
    "   ![dog_swap](img_embed/crop_swap.PNG \"dog\")\n",
    "2. Cover the face of lena with white mask as shown as the following:\n",
    "\n",
    "   ![lena_mask](img_embed/lena_mask.PNG \"lena mask\")\n",
    "\n",
    "3. Extract the region of interest (flower) from the 'flower.jfif'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e201988-a45e-41a7-80a3-d1d6ad93f1cf",
   "metadata": {},
   "source": [
    "# Color space / color model\n",
    "In the most common color space, RGB (Red Green Blue), colors are represented in terms of their red, green, and blue components. In more technical terms, RGB describes a color as a tuple of 3 components. Each component can take a value between 0 and 255, where the tuple (0, 0, 0) represents black and (255, 255, 255) represents white. RGB is considered an \"additive\" color space, and colors can be imagined as being produced from shining quantities of red, blue and green light onto a black background.\n",
    "\n",
    "There are so many color spaces because different color spaces are useful for different purposes. We will typically work with 3 kinds of color models: blue-green-red (BGR), grayscale and hue-saturation-value (HSV).\n",
    "\n",
    "In reality, color is a continuous phenomenon, meaning that there are an infinite number of colors. Color spaces, however represent color through discrete structures (a fixed number of whole number integer values), which is acceptable since the human eye and perception are also limited. \n",
    "\n",
    "## RGB color space\n",
    "It is an additive colorspace where colors are obtained by a linear combination of Red, Green and Blue values. \n",
    "\n",
    "There are some inherent problems asociated with RGB colorspace:\n",
    "- significant perceptual non-uniformity.\n",
    "- mixing of chrominance and luminance data.\n",
    "\n",
    "## LAB color space\n",
    "3 components:\n",
    "- L: lightness (intensity).\n",
    "- A: color component ranging from green to red.\n",
    "- B: color component ranging from blue to yellow.\n",
    "\n",
    "The L component is independent of color information and encodes brightness only. \n",
    "\n",
    "## YCrCb color space\n",
    "Like LAB colorspace, the luminance and chrominances are separated into different channels. Y is used to represent luminance (or luma), Cb represents blue-difference, and Cr represents red-difference.\n",
    "\n",
    "## HSV color space\n",
    "1. Hue is the color attribute that describes pure color. \n",
    "2. Saturation is the quantity that reflect the degree to which pure color is diluted by white light.\n",
    "3. Value or intensity is brightness.\n",
    "\n",
    "![hsv cylindrical spectrum](img_embed/hsv_cylindrical.jfif \"HSV\")\n",
    "\n",
    "> **Important** 🗝️\n",
    ">\n",
    "> In OpenCV, hue range is $[0,179]$, saturation range is $[0,255]$, and value range is $[0,255]$. Different software use different scales. So if you are comparing OpenCV values with them, you need to **normalize these ranges**. \n",
    "> The important takeaway is: *Lighting condition can mean the difference between success and failure of your computer vision algorithm.* Thus, color space which could factor out luminance into different channel should be the choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9bc8a-4feb-402e-88b2-20c4d0a89139",
   "metadata": {},
   "source": [
    "## Splitting and merging Image channels\n",
    "The B, G and R channels can be split into their individual planes when needed. Then the individual channels can be merged back together to form BGR image again. The splitting and merging operations can be attained by the following functions respectively:\n",
    "* `cv.split(m)`, where m is a multi-channel array.\n",
    "* `cv.merge(mv)`, where mv is a tuple / list of matrices to be merged; all the matrices in mv must have the same size and the same depth (precision of each pixel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0da871-8964-413b-83ba-d47f1d86b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, g, r = cv.split(img)\n",
    "img_merge = cv.merge((b, g, r))\n",
    "\n",
    "print(f\"Are the two images the same? {np.equal(img, img_merge).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1625f56-7bd9-40b3-9d48-b6b579bcee5c",
   "metadata": {},
   "source": [
    "## Manipulate image channels\n",
    "\n",
    "### HSV color channels\n",
    "\n",
    "#### Hue channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbfda1-b4a9-4556-a5e1-b3d9ce5af446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hue to a certain value\n",
    "img = cv.imread(\"images/meal.jpg\")\n",
    "img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "h, s, v = cv.split(img_hsv)\n",
    "h_new = np.zeros_like(h) + 30\n",
    "h_new = np.uint8(h_new)\n",
    "\n",
    "transform = cv.merge((h_new, s, v))\n",
    "transform_display = cv.cvtColor(transform, cv.COLOR_HSV2BGR)\n",
    "\n",
    "display_images([img, transform_display], (\"original\", \"hue=30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba3ce6-74c4-4f0d-a783-84eb91e50268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the saturation \n",
    "img = cv.imread(\"images/meal.jpg\")\n",
    "img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "h, s, v = cv.split(img_hsv)\n",
    "s_new = np.zeros_like(s) + 255\n",
    "s_new = np.uint8(s_new)\n",
    "\n",
    "transform = cv.merge((h, s_new, v))\n",
    "transform_display = cv.cvtColor(transform, cv.COLOR_HSV2BGR)\n",
    "\n",
    "display_images([img, transform_display], (\"original\", \"saturation decrease\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58f6e8-d02f-487a-a282-5486911a4d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46bfbe4-83a3-46aa-87d3-3dfd04769708",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Display the blue, green and red channel of the **lena** image simultaneously. Comment on the images displayed.\n",
    "2. Display the following images which originates from the file \"images/dog.jfif\".\n",
    "\n",
    "![exercise_fig](img_embed/diff_color_channels.jpg \"color dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1577661-fa8f-4fde-be95-506663a9639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "img = cv.imread(\"images/lena.jfif\")\n",
    "\n",
    "colors = (\"blue\", \"green\", \"red\")\n",
    "channels = cv.split(img)\n",
    "\n",
    "for i, channel in enumerate(channels):\n",
    "    new_channel = np.zeros_like(channel) + 255\n",
    "    new_channel = np.uint8(new_channel)\n",
    "    img_copy = img.copy()\n",
    "    img_copy[..., i] = new_channel\n",
    "    cv.imshow(colors[i], img_copy)\n",
    "    \n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c2cf7-2dca-43f0-aed8-e607158d0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question2\n",
    "img = cv.imread(\"images/dog.jfif\")\n",
    "\n",
    "colors = (\"blue\", \"green\", \"red\")\n",
    "channels = cv.split(img)\n",
    "\n",
    "for i, channel in enumerate(channels):\n",
    "    new_channel = np.zeros_like(channel) + 255\n",
    "    new_channel = np.uint8(new_channel)\n",
    "    img_copy = img.copy()\n",
    "    img_copy[..., i] = new_channel\n",
    "    cv.imshow(colors[i], img_copy)\n",
    "    \n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004d398-06ec-49e1-a937-c4f41e8c2be5",
   "metadata": {},
   "source": [
    "# Point operator \n",
    "\n",
    "## Point operator\n",
    "It is merely mathematical operations on each pixel value of an image. The formula is as follows:\n",
    "$$f(x, y) = \\alpha f(x, y) + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a821eb2-450b-4b3c-aa4a-f01632897bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the point operator function (multiplication, addition, subtraction and division)\n",
    "def point_op(img, alpha, beta):\n",
    "    \"\"\"Point operator function\n",
    "    Argument:\n",
    "    ---\n",
    "    img: input image\n",
    "    alpha: coefficient\n",
    "    beta: bias\n",
    "    \n",
    "    Returns:\n",
    "    ---\n",
    "    Unsigned 8-bit image array\"\"\"\n",
    "    img = img.astype(np.float32)\n",
    "    res = alpha * img + beta\n",
    "    # clip the pixel values \n",
    "    res = np.clip(res, 0, 255)\n",
    "    return np.uint8(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c652846-620d-4532-8083-8037cbc1423d",
   "metadata": {},
   "source": [
    "### Enhance the contrast and brighten the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9d436-468a-4708-89fb-d5a2c90c0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv.imread('images/camera.jpg', 0)\n",
    "transform = point_op(gray, 2, 0)\n",
    "\n",
    "display_images([gray, transform], (\"grayscale\", \"transform\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568022f-0c59-4d12-afc8-372c2ff8a90b",
   "metadata": {},
   "source": [
    "### Lower the contrast and darken the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6bbad-80c3-4e0c-a514-08e56e197ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = point_op(gray, 1, -50)\n",
    "\n",
    "display_images([gray, transform2], (\"grayscale\", \"darken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192496f1-9be0-4956-a8aa-7cc1e0935779",
   "metadata": {},
   "source": [
    "## Weekly activity\n",
    "1. Create a **random noise color and grayscale** image. You can set your own width and height, but keep the total number of pixels of both images identical.\n",
    "2. Convert the code chunk found under section <a href=\"#Section1\">Divide an image into smaller patches using cropping</a> into a function with the following signature:\n",
    "```python\n",
    "crop_grid(img, num_horizontal_grid, num_vertical_grid, line_color)\n",
    " # img is the source image\n",
    " # num_horizontal_grid and num_vertical_grid are the number of patches along x and y axes.\n",
    " # line_color is the color of the grid line.\n",
    " # The output of the function should be image with grids\n",
    "```\n",
    "3. How would you *change the brightness* of a **color image**? Suggest **two ways** to perform the image processing operations. Implement your methods by providing the example codes. You are free to choose any image.\n",
    "4. Provide at least one common use cases for the following color spaces:\n",
    "    - RGB\n",
    "    - HSV\n",
    "    - CIELAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef5805-dc0d-4964-9f8e-4c133a94ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question1\n",
    "noise_color_img = np.random.randint(0, high=256, size = (100, 100, 3), dtype=np.uint8)\n",
    "gray_img = np.random.randint(0, high=256, size = (100, 100, 3), dtype=np.uint8)\n",
    "plt.imshow(noise_color_img)\n",
    "plt.imshow(gray_img)\n",
    "plt.title(\"random noise color image\")\n",
    "plt.title(\"random noise gray image\")\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5dbf6-a76b-4fbd-9b22-9467f0a8dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_img = np.random.randint(0, 256, size = (100, 100), dtype=np.uint8)\n",
    "plt.imshow(gray_img, cmap='gray')\n",
    "plt.title(\"random noise gray image\")\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b639607-0d5f-4c85-ae11-c03b44e5dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question2\n",
    "\n",
    "def crop_grid(img, num_horizontal_patches, num_vertical_patches, line_color):\n",
    "    img_copy = img.copy()\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    patch_height = height // num_vertical_patches\n",
    "    patch_width = width // num_horizontal_patches\n",
    "\n",
    "    for i in range(1, num_horizontal_patches):\n",
    "        x = i * patch_width\n",
    "        cv.line(img_copy, (x, 0), (x, height), line_color, 1)\n",
    "\n",
    "    for j in range(1, num_vertical_patches):\n",
    "        y = j * patch_height\n",
    "        cv.line(img_copy, (0, y), (width, y), line_color, 1)\n",
    "\n",
    "    return img_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debfdbd4-23fb-4648-a203-eac9fe568861",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'point_op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      4\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mpoint_op\u001b[49m(img, alpha, beta)\n\u001b[0;32m      6\u001b[0m display_images([img, res], (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, beta=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'point_op' is not defined"
     ]
    }
   ],
   "source": [
    "#Question3 method 1\n",
    "img = cv.imread(\"images/camera.jpg\")\n",
    "alpha = 2\n",
    "beta = 0\n",
    "res = point_op(img, alpha, beta)\n",
    "display_images([img, res], (\"original\", f\"alpha={alpha}, beta={beta}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fee28125-dd0e-431f-9e8e-c8a79d816169",
   "metadata": {},
   "outputs": [],
   "source": [
    "##method2\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def adjust_brightness(image, factor):\n",
    "    normalized_img = image / 255.0\n",
    "    brightened_img = np.clip(normalized_img * factor, 0, 1)\n",
    "    return (brightened_img * 255).astype(np.uint8)\n",
    "\n",
    "img = cv.imread(\"images/camera.jpg\")\n",
    "brightness_factor = 1.2\n",
    "brightened_img = adjust_brightness(img, brightness_factor)\n",
    "\n",
    "cv.imshow('Original Image', img)\n",
    "cv.imshow('Brightened Image', brightened_img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8db17c-7f98-421e-8477-c930b24eb3dc",
   "metadata": {},
   "source": [
    "# Q4\n",
    "## USE CASE \n",
    "RGB - Computer Graphics\n",
    "HSV - Image Processing and Analysis\n",
    "CIELAB - Color Difference Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29006417-2da2-47fb-b407-99f91d93877e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
